**Этап 5**

Wrk + profiler традиционно можно глянуть вот тут - https://drive.google.com/open?id=1e02wKAtnpyoT0oKZhF6l6eaOUMuxxUjW
Добавлен тест на метод replicas у топологии.

На данном этапе, за счет добавления работы по сети, profiler показал, что почти 10-12% времени тратится в среднем на проксирование запросов и почти столько же запросы к базе данных. 
(В зависимости от нагрузок - только put/get или смешанная) - здесь можно рассмотреть вариант с реактивными стримами для оптимизации.

Wrk показал, что в целом, по сравнению с предыдущим этапом, показатели не изменились - (#85 (comment)). 
Одна из нод незначительно проседает по сравнению с другими, что, возможно, говорит о том что реализованный мною алгоритм хэширования работает не совсем оптимально.

**Этап 6**

На этом этапе добавился HttpClient из стандартной Java-библиотеки, что позволило нагружать сервер несколькими десятками соединений.

Один экземпляр клиента используется внутри EntityService для обработки прокси-запросов к другим нодам, для CRUD-операции в самом хранилище ноды используется внутенний пул воркеров. 
Результаты всех обработок, которые сейчас представлены в виде класса CompletableFuture, объединяются и формируется Response, который потом обрабатывается асинхронно в самом контроллере.

По wrk можно сказать, что в случае put-запросов очень резко и явно наблюдаются просаживания в районе ~97-99 перцентилли. 
Аналогичная ситуация наблюдается и при GET-запросах, где средняя медиана уже составляет примерно 18-23ms(но явно видно, что результаты на 97-99 перцентилли в среднем равны медиане).
Возможная причина - единный экземпляр HttpClient на сервис.

Flame-graph на этой стадии показывает, что много аллокации в памяти (почти 14-24% от общего - в зависимости от запроса и типов нагрузки) и много времени (7-13%) занимает обработка фьюч.
В случае GET- запросов также сюда прибавляется поиск в хранилище и аллокация Iterator / ByteBuffer.   